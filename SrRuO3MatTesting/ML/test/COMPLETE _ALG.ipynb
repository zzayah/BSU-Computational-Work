{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.calibration import LinearSVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# Different feature selection methods\n",
    "import sklearn.feature_selection as fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input SCF Data and Coordinates:\n",
    "data = pd.read_csv('data/SCF_data.csv')\n",
    "\n",
    "# Input elements | Ex. types_elements_init = ['Sr','Ti', 'O', 'He']\n",
    "types_elements_init = []\n",
    "\n",
    "# Input grid increment | Ex. grid_increment = 1 for a 7 x 7, 49 grid point system\n",
    "grid_inc = 0\n",
    "\n",
    "# Input amount of grid points, should be a square | Ex. grid_points = 49\n",
    "grid_points = 0\n",
    "\n",
    "# Be sure to input top_features (will be prompted later)\n",
    "top_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variable initialization\n",
    "features = data.iloc[:, 4:]\n",
    "SCFshift = data.iloc[:, 3:4]\n",
    "xy = data.iloc[:, :2]\n",
    "topFeatures = data[top_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(rij, r_cut, type_func):\n",
    "    if type_func == 'cos':\n",
    "        if rij <= r_cut:\n",
    "            return 0.5*(np.cos(np.pi*rij/r_cut) + 1.0)\n",
    "        elif rij > r_cut:\n",
    "            return 0.0\n",
    "    elif type_func == 'tanh':\n",
    "        if rij <= r_cut:\n",
    "            return np.tanh(1-rij/r_cut)**3.0\n",
    "        elif rij > r_cut:\n",
    "            return 0.0\n",
    "\n",
    "\n",
    "def get_eta_values(rcut, n, min_val, max_val):\n",
    "    eta = np.zeros((2*(n+1)), dtype=float)\n",
    "    rs = np.zeros((2*(n+1)), dtype=float)\n",
    "    ran = max_val - min_val\n",
    "    def R_sm(i, m): return rcut/(i**(m/i))\n",
    "    for i in range(n + 1):\n",
    "        # eta[i] = 2/( (i+1)*ran/( n + 1 ) )**2.0\n",
    "        eta[i] = (1.0/R_sm(n + 1, i))**2.0\n",
    "        rs[i] = 0.0\n",
    "    for i in range(n + 1):\n",
    "        rs[n+i+1] = R_sm(n + 1, i)\n",
    "        eta[2*(n+1)-i-1] = 1.0/(R_sm(n + 1, n+1-i) - R_sm(n + 1, n-i))**2.0\n",
    "    return eta, rs\n",
    "\n",
    "\n",
    "def G1(distances, r_cut, type_func, types_elements, elements):\n",
    "    nvirtual = elements.count('He')\n",
    "    nreal = len(elements)-nvirtual\n",
    "    g1desc = np.zeros((nvirtual, len(types_elements)-1), dtype=float)\n",
    "    for i, typei in enumerate(elements):\n",
    "        if typei == 'He':  # only compute G1 for virtual atoms\n",
    "            for j, typej in enumerate(elements):\n",
    "                if typej != 'He':  # only compute G1 wrt real atoms\n",
    "                    rij = distances[i, j]\n",
    "                    idx = types_elements.index(elements[j])\n",
    "                    g1desc[i-nreal][idx] += fc(rij, r_cut, type_func)\n",
    "    return g1desc\n",
    "# For G2 and G3, each row has the ith atoms descriptors\n",
    "# and each column are the descriptors for different eta/kappa values\n",
    "#            eta1       eta2     ...\n",
    "# atom1  [ desc1,1    desc1,2     ... ]\n",
    "# atom2  [ desc2,1    desc2,2     ... ]\n",
    "\n",
    "\n",
    "def G2(distances, r_cut, eta, rs, type_func, types_elements, elements):\n",
    "    nvirtual = elements.count('He')\n",
    "    # Dummy atoms were appended to the end of the list; nreal shows the length of real atoms when deleting the # of grid points from the count\n",
    "    nreal = len(elements)-nvirtual\n",
    "    # len(types) -1 represents the dimension of the real atoms only (there are two atom types here, real and dummies)\n",
    "    g2desc = np.zeros((nvirtual, len(eta), len(types_elements)-1), dtype=float)\n",
    "   # print(nreal,nvirtual)\n",
    "    eta = np.array(eta)\n",
    "    for i, typei in enumerate(elements):\n",
    "        if typei == 'He':\n",
    "            for j, typej in enumerate(elements):\n",
    "                if typej != 'He':  # only compute G1 with respect to real atoms;\n",
    "                    rij = distances[i, j]\n",
    "                    f_fun = fc(rij, r_cut, type_func)\n",
    "                   # rs_fac = np.multiply( -1, np.square( np.subtract( rij, rs ) ) )\n",
    "                    # np.multiply( eta, rs_fac ) )\n",
    "                    exp_fac = np.exp(-eta*(rij-rs)**2)\n",
    "                    idx = types_elements.index(elements[j])\n",
    "                    g2desc[i-nreal, :, idx] += exp_fac * \\\n",
    "                        f_fun  # np.multiply(exp_fac, f_fun)\n",
    "\n",
    "    return g2desc\n",
    "\n",
    "\n",
    "def G3(distances, r_cut, kappa, type_func, types_elements, elements):\n",
    "    nvirtual = elements.count('He')\n",
    "    g3desc = np.zeros(\n",
    "        (nvirtual, len(kappa), len(types_elements)-1), dtype=float)\n",
    "    kappa = np.array(kappa)\n",
    "\n",
    "    # Dummy atoms were appended to the end of the list; nreal shows the length of real atoms when deleting the # of grid points from the count\n",
    "    nreal = len(elements)-nvirtual\n",
    "    # len(types) -1 represents the dimension of the real atoms only (there are two atom types here, real and dummies)\n",
    "    g3desc = np.zeros(\n",
    "        (nvirtual, len(kappa), len(types_elements)-1), dtype=float)\n",
    "    # print(nreal,nvirtual)\n",
    "\n",
    "    for i, typei in enumerate(elements):\n",
    "        if typei == 'He':\n",
    "            for j, typej in enumerate(elements):\n",
    "                if typej != 'He':  # only compute G1 with respect to real atoms;\n",
    "                    rij = distances[i, j]\n",
    "                    f_fun = fc(rij, r_cut, type_func)\n",
    "                    # rs_fac = np.multiply( -1, np.square( np.subtract( rij, rs ) ) )\n",
    "                    # np.multiply( eta, rs_fac ) )\n",
    "                    cos_fac = np.cos(kappa * rij)\n",
    "                    idx = types_elements.index(elements[j])\n",
    "                    g3desc[i-nreal, :, idx] += cos_fac * \\\n",
    "                        f_fun  # np.multiply(exp_fac, f_fun)\n",
    "\n",
    "    return g3desc\n",
    "# Lambda can have values of +1 or -1\n",
    "# zeta can have integer values greater than zero\n",
    "\n",
    "\n",
    "def G4(distance_vectors, distances, r_cut, type_func, elements, combo_types):\n",
    "    # init_size = list( np.shape( bp_calcs ) )\n",
    "    # init_size.append( len( type_combos ) )\n",
    "    nvirtual = elements.count('He')\n",
    "    ncombos = len(combo_types)\n",
    "    g4desc = np.zeros((nvirtual, len(zeta), ncombos), dtype=float)\n",
    "    # combos = np.array( combos )\n",
    "    lambd = np.array([1.0])\n",
    "    for i, typei in enumerate(elements):\n",
    "        if typei == 'He':\n",
    "            for j, typej in enumerate(elements):\n",
    "                if typej != 'He':\n",
    "\n",
    "                    rij = distances[i, j]\n",
    "                    rij_vec = distance_vectors[i, j]\n",
    "\n",
    "                    for k, typek in enumerate(elements):\n",
    "                        if typek != 'He':\n",
    "                            if j == k:\n",
    "                                continue\n",
    "                            rik = distances[i, k]\n",
    "                            rik_vec = distance_vectors[i, k]\n",
    "\n",
    "                            rjk = distances[j, k]\n",
    "                            rjk_vec = distance_vectors[j, k]\n",
    "                            for l, m in enumerate(zeta):\n",
    "\n",
    "                                cost = np.dot(rij_vec, rik_vec)/(rij*rik)\n",
    "\n",
    "                                ang_lin = (2.0**(1-m))*(1+lambd*cost)**m\n",
    "\n",
    "                                exp_fac = np.exp(-eta[0]\n",
    "                                                 * (rij**2+rik**2+rjk**2))\n",
    "\n",
    "                                fc_fac = fc(rij, r_cut, type_func)*fc(rik,\n",
    "                                                                      r_cut, type_func)*fc(rjk, r_cut, type_func)\n",
    "\n",
    "                                if [typej, typek] in combo_types:\n",
    "                                    idx = combo_types.index([typej, typek])\n",
    "                                else:\n",
    "                                    idx = combo_types.index([typek, typej])\n",
    "                                g4desc[i-nreal, l, idx] += ang_lin * \\\n",
    "                                    exp_fac*fc_fac\n",
    "    return g4desc\n",
    "# Lambda can have values of +1 or -1\n",
    "# zeta can have integer values greater than zero\n",
    "\n",
    "\n",
    "def G5(distance_vectors, distances, r_cut, type_func, elements, combo_types):\n",
    "    # init_size = list( np.shape( bp_calcs ) )\n",
    "    # init_size.append( len( type_combos ) )\n",
    "    nvirtual = elements.count('He')\n",
    "    ncombos = len(combo_types)\n",
    "    g5desc = np.zeros((nvirtual, len(zeta), ncombos), dtype=float)\n",
    "    # combos = np.array( combos )\n",
    "    lambd = np.array([1.0])\n",
    "    for i, typei in enumerate(elements):\n",
    "        if typei == 'He':\n",
    "            for j, typej in enumerate(elements):\n",
    "                if typej != 'He':\n",
    "\n",
    "                    rij = distances[i, j]\n",
    "                    rij_vec = distance_vectors[i, j]\n",
    "\n",
    "                    for k, typek in enumerate(elements):\n",
    "                        if typek != 'He':\n",
    "                            if j == k:\n",
    "                                continue\n",
    "                            rik = distances[i, k]\n",
    "                            rik_vec = distance_vectors[i, k]\n",
    "\n",
    "                            rjk = distances[j, k]\n",
    "                            rjk_vec = distance_vectors[j, k]\n",
    "                            for l, m in enumerate(zeta):\n",
    "\n",
    "                                cost = np.dot(rij_vec, rik_vec)/(rij*rik)\n",
    "\n",
    "                                ang_lin = (2.0**(1-m))*(1+lambd*cost)**m\n",
    "\n",
    "                                exp_fac = np.exp(-eta[0]*(rij**2+rik**2))\n",
    "\n",
    "                                fc_fac = fc(rij, r_cut, type_func) * \\\n",
    "                                    fc(rik, r_cut, type_func)\n",
    "\n",
    "                                if [typej, typek] in combo_types:\n",
    "                                    idx = combo_types.index([typej, typek])\n",
    "                                else:\n",
    "                                    idx = combo_types.index([typek, typej])\n",
    "                                g5desc[i-nreal, l, idx] += ang_lin * \\\n",
    "                                    exp_fac*fc_fac\n",
    "    return g5desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.io import read\n",
    "from ase.visualize import view\n",
    "from ase.build import add_adsorbate\n",
    "from ase.io import write\n",
    "\n",
    "# convert data csv to xyz\n",
    "ase.write('data.xyz', data, format='xyz')\n",
    "\n",
    "total_ase_atoms = read('data.xyz')\n",
    "\n",
    "#iterate through a 6 by 6 equally space grid of adsorption sites, with one corner at 0, 0, and one corner at 7.63363 , 7.63363. Add an adsorbate at each site as well as the opposite height of the entire slab.\n",
    "# 7.63363/7 = 1.0905185714285713\n",
    "# 7.63363/6 = 1.2722725\n",
    "\n",
    "#write np code that takes the square root of 64\n",
    "\n",
    "\n",
    "xx_reshape = np.zeros((64, 1))\n",
    "yy_reshape = np.zeros((64, 1))\n",
    "for i in range(0, np.sqrt(grid_points)):\n",
    "    for j in range(0, np.sqrt(grid_points)):\n",
    "        print(i*grid_inc, j*grid_inc)\n",
    "        xx_reshape[i*np.sqrt(grid_points)+j] = i*grid_inc\n",
    "        yy_reshape[i*np.sqrt(grid_points)+j] = j*grid_inc\n",
    "        \n",
    "        add_adsorbate(total_ase_atoms, 'He', 3, (i*grid_inc, j*grid_inc))\n",
    "        \n",
    "        # energy_total = model.get_potential_energy()\n",
    "\n",
    "view(total_ase_atoms, viewer='x3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_vectors=total_ase_atoms.get_all_distances(mic=True,vector=True)\n",
    "distances=total_ase_atoms.get_all_distances(mic=True)\n",
    "types_elements = types_atoms_init\n",
    "elements = total_ase_atoms.get_chemical_symbols()\n",
    "nvirtual=elements.count('He')\n",
    "nreal=len(elements)-nvirtual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_func='cos'\n",
    "r_cut = 12.0\n",
    "eta,rs=get_eta_values(12,6,1,6)\n",
    "kappa=[0.5, 1.0, 1.5, 2.0]\n",
    "zeta=[1,2,4,8,16,32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridpoints=pd.DataFrame(xx_reshape,columns=['X'])\n",
    "gridpoints['Y']=yy_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2=G2(distances,r_cut, eta, rs, type_func, types_elements, elements)\n",
    "gridpoints['G2-0']=g2[:,0,0]\n",
    "gridpoints['G2-1']=g2[:,1,0]\n",
    "gridpoints['G2-2']=g2[:,2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g3=G3(distances, r_cut, kappa, type_func, types_elements, elements)\n",
    "gridpoints['G3-0']=g3[:,0,0]\n",
    "gridpoints['G3-1']=g3[:,1,0]\n",
    "gridpoints['G3-2']=g3[:,2,0]\n",
    "gridpoints['G3-3']=g3[:,3,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncombos = (len(types_elements) - 1)*len(types_elements)//2\n",
    "combo_types = []\n",
    "for i in range(len(types_elements)-1):\n",
    "    for j in range(i,len(types_elements)-1):\n",
    "        combo_types.append([types_elements[i],types_elements[j]])\n",
    "g4=G4(distance_vectors,distances,r_cut,type_func,elements,combo_types)\n",
    "gridpoints['G4-0']=g4[:,0,0]\n",
    "gridpoints['G4-1']=g4[:,1,0]\n",
    "gridpoints['G4-2']=g4[:,2,0]\n",
    "gridpoints['G4-3']=g4[:,3,0]\n",
    "gridpoints['G4-4']=g4[:,4,0]\n",
    "gridpoints['G4-5']=g4[:,5,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALIZED DATA\n",
    "\n",
    "def perform_clustering_norm(features, SCFshift, algorithm_index,n_clusters):\n",
    "    # Generate combinations of variables\n",
    "    variable_combinations = list(itertools.combinations(features.columns, 2))\n",
    "    total_combinations = len(variable_combinations)\n",
    "\n",
    "    # Select the algorithm based on the given index\n",
    "    algorithms = [LinearSVC(), KMeans(n_clusters=n_clusters)]\n",
    "    algorithm = algorithms[algorithm_index]\n",
    "\n",
    "    # Initialize an empty list to store results\n",
    "    result_list = []\n",
    "\n",
    "    # Normalize the feature and SCFshift data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n",
    "    scaled_SCFshift = scaler.fit_transform(SCFshift)\n",
    "\n",
    "    # Perform clustering on each combination of variables\n",
    "    for i, combination in enumerate(variable_combinations):\n",
    "        variables = scaled_features[list(combination)]\n",
    "\n",
    "        # Fit the clustering algorithm to the data\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            algorithm.fit(variables)\n",
    "\n",
    "        # Overlay clusters on scaled SCFshift data\n",
    "        labels = algorithm.labels_\n",
    "        score = silhouette_score(scaled_SCFshift, labels)\n",
    "\n",
    "        # Store the results in the list\n",
    "        result_list.append({'Variable 1': combination[0],\n",
    "                            'Variable 2': combination[1],\n",
    "                            'Silhouette Score': score})\n",
    "\n",
    "        # Update progress\n",
    "        progress = (i + 1) / total_combinations * 100\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(f\"Clustering Progress: [{progress:.1f}%] \")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # Convert the list to a DataFrame\n",
    "    result_df = pd.DataFrame(result_list)\n",
    "\n",
    "    # Sort the DataFrame by silhouette score in descending order\n",
    "    result_df = result_df.sort_values(by='Silhouette Score', ascending=False)\n",
    "\n",
    "    # Select the top 100 variables\n",
    "    top_100_variables = result_df.head(100)\n",
    "\n",
    "    #convert top 100 variables to csv named \"top100norm.csv\"\n",
    "    top_100_variables.to_csv('top100norm.csv')\n",
    "\n",
    "    return top_100_variables\n",
    "\n",
    "\n",
    "# Example usage\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    top_variables = perform_clustering_norm(features, SCFshift, 1, 4)\n",
    "\n",
    "#save top variables to a csv file named \"oxygen_abs_top_var.csv\"\n",
    "top_variables.to_csv('oxygen_abs_top_var_4_norm.csv')\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    top_variables = perform_clustering_norm(features, SCFshift, 1, 3)\n",
    "\n",
    "top_variables.to_csv('oxygen_abs_top_var_3_norm.csv')\n",
    "\n",
    "print(top_variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input top features | Ex. top_features = ['G4-3  R_cut = 8', 'G5-5  R_cut = 11']\n",
    "top_features_selected = []\n",
    "\n",
    "top_feature_df = data[top_features_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a DataFrame with the variables from top_feat\n",
    "variables_df = data[top_features]\n",
    "\n",
    "# Perform clustering\n",
    "kmeans = KMeans(n_clusters=4)  # Set the desired number of clusters\n",
    "kmeans.fit(variables_df)\n",
    "\n",
    "# Assign cluster labels to each index of the SCF values\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "data['Cluster_labels'] = kmeans.labels_\n",
    "\n",
    "# Obtain the SCFshift values\n",
    "scf_shift_values = data['SCFshift']\n",
    "\n",
    "# Generate x and y values\n",
    "x_values = np.linspace(0, 7.8254, 8)  # Adjusted to match the number of points in the original code\n",
    "y_values = np.linspace(0, 7.8254, 8)  # Adjusted to match the number of points in the original code\n",
    "\n",
    "# Generate all combinations of x and y coordinates\n",
    "coordinates = [(x, y) for x in x_values for y in y_values]\n",
    "\n",
    "# Create a colormap plot based on SCFshift values\n",
    "plt.imshow(scf_shift_values.values.reshape(8, 8), cmap='viridis', extent=[0, 7.8254, 0, 7.8254], origin='lower', interpolation='bicubic')\n",
    "\n",
    "# Add cluster numbers as text annotations at the center of each coordinate\n",
    "for i, coord in enumerate(coordinates):\n",
    "    if cluster_labels[i] == 0:\n",
    "        plt.text(coord[0], coord[1], str(cluster_labels[i]), ha='center', va='center', color='blue')\n",
    "    elif cluster_labels[i] == 1:\n",
    "        plt.text(coord[0], coord[1], str(cluster_labels[i]), ha='center', va='center', color='white')\n",
    "    elif cluster_labels[i] == 2:\n",
    "        plt.text(coord[0], coord[1], str(cluster_labels[i]), ha='center', va='center', color='red')\n",
    "    elif cluster_labels[i] == 3:\n",
    "        plt.text(coord[0], coord[1], str(cluster_labels[i]), ha='center', va='center', color='yellow')\n",
    "\n",
    "# Remove x and y axis\n",
    "plt.axis('off')\n",
    "\n",
    "colorbar = plt.colorbar()\n",
    "colorbar.ax.set_ylabel('SCF Shift (Ry)')\n",
    "\n",
    "# Adjust the spacing between the plot and the title\n",
    "plt.subplots_adjust(top=0.9)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Import the data from CSV\n",
    "data = pd.read_csv('SCFshift.csv', header=None)\n",
    "SCFshift = data.iloc[:, 0].values\n",
    "\n",
    "# Check the number of unique values\n",
    "unique_values = np.unique(SCFshift)\n",
    "num_unique = len(unique_values)\n",
    "\n",
    "if num_unique < 2:\n",
    "    print(\"Error: Number of unique values is less than 2. Check your data.\")\n",
    "    exit()\n",
    "\n",
    "# Perform clustering and calculate silhouette scores\n",
    "silhouette_scores = []\n",
    "clusters = range(2, min(num_unique, 15))\n",
    "\n",
    "for n_clusters in clusters:\n",
    "    # Fit the K-means model\n",
    "    kmeans = KMeans(max_iter=100, n_clusters=n_clusters)\n",
    "    kmeans.fit(SCFshift.reshape(-1, 1))\n",
    "\n",
    "    # Predict cluster labels\n",
    "    labels = kmeans.predict(SCFshift.reshape(-1, 1))\n",
    "\n",
    "    # Calculate silhouette score\n",
    "    score = silhouette_score(SCFshift.reshape(-1, 1), labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.plot(clusters, silhouette_scores)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs Number of Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_regression = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [0, 0, 0, 0]\n",
    "\n",
    "data['index'] = range(0, len(data))\n",
    "\n",
    "for i in range(0, 63):\n",
    "    if data_for_regression['Cluster_labels'][i] == 0 and rows[0] == 0:\n",
    "        rows[0] = data_for_regression['index'][i]\n",
    "    elif data['Cluster_labels'][i] == 1 and rows[1] == 0:\n",
    "        rows[1] = data_for_regression['index'][i]\n",
    "    elif data['Cluster_labels'][i] == 2 and rows[2] == 0:\n",
    "        rows[2] = data_for_regression['index'][i]\n",
    "    elif data['Cluster_labels'][i] == 3 and rows[3] == 0:\n",
    "        rows[3] = data_for_regression['index'][i]\n",
    "\n",
    "# make rows a 1D array\n",
    "rows = np.array(rows)\n",
    "\n",
    "# Input top features | Ex. top_features = ['G4-3  R_cut = 8', 'G5-5  R_cut = 11']\n",
    "indiv_feature_ary = []\n",
    "\n",
    "selected_feature_df = data[top_features_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_scores = []\n",
    "degrees = []\n",
    "\n",
    "for i in range(1, 10):\n",
    "    # Take specific rows of the DataFrame for the training set\n",
    "    selected_vars = selected_feature_df.iloc[rows]\n",
    "    selected = data_for_regression.iloc[rows]\n",
    "\n",
    "    # Create DataFrames with the rest of the rows for the testing set\n",
    "    unselected_vars = data_for_regression[indiv_feature_ary].drop(selected_vars.index)\n",
    "    unselected = data_for_regression.drop(selected.index)\n",
    "\n",
    "    # Create polynomial features for the training and testing sets\n",
    "    degree = i  # You can adjust the degree as needed\n",
    "    poly_features = PolynomialFeatures(degree=degree)\n",
    "    X_train = poly_features.fit_transform(selected_vars)\n",
    "    X_test = poly_features.transform(unselected_vars)\n",
    "\n",
    "    # Prepare the target variables for training and testing sets\n",
    "    y_train = selected[\"SCFshift\"].values\n",
    "    y_test = unselected[\"SCFshift\"].values\n",
    "\n",
    "    # Create and train the linear regression model\n",
    "    regressor = LinearRegression()\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing set\n",
    "    y_pred = regressor.predict(X_test)\n",
    "\n",
    "    # Evaluate the model using R-squared (R²) score\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    r2_scores.append(r2)\n",
    "    degrees.append(degree)\n",
    "\n",
    "# plot r2 on the y and degrees on the x\n",
    "plt.plot(degrees, r2_scores)\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('R2 Score')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
